- study the reward function : penalize inaction necessary ?
- DQN : look at the different stages of DQN (see lab6, then dper by deemind)
    Horgan, Dan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver.
    ‘Distributed Prioritized Experience Replay’. arXiv, 2 March 2018. https://doi.org/10.48550/arXiv.1803.00933.
- look at similar environment : https://paperswithcode.com/sota/atari-games-on-atari-2600-asteroids.
- redundancy in introduction ? 
- determining the observation in itself is a great aspect of the project
- do not redefine Qtable and epsilon greedy strategy
- manhattan distance formula
- change code comments to english (careful with french !)
- discussion : random agent doesnt rely on observarion space so it has no influence whatsoever
- discussion on challenges faced by agents in the complex environment (what failed, what had to be changed)
- compare behaviors when constraints : red zone on the map, different levels, additional variables to
    influence decison-making => adaptability of the agent
- integrate literature
- details on the experimental part : number of iterations, measures, hyperparameters
- detail the theoretical framework to develop the agent
- formal definition and formulas of Q table and epsilon greedy strategy
- if talk about the java game, tell how it influences our work
- pseudo code / algorithms of how we used Q table and epsilon greedy
- use visual representation more wisely in the code
- evaluation methods to test environment's adequacy : description, data, visuals
- 